{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMVi6ab3m24S73TyoT3jK5e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# TOKENS Y EMBEDDINGS\n","\n","En esta lección veremos qué son los tokens y los embeddings, dos conceptos clave al momento de representar el texto que ingresa a un LLM.\n","\n","Además, tendremos una primera aproximación a un LLM y veremos en acción estos dos conceptos a través de un ejemplo práctico. Y adicionalmente, al final de la lección hablaremos de la **ventana de contexto**, un concepto también relacionado con los tokens."],"metadata":{"id":"zu2T6cv63sFG"}},{"cell_type":"markdown","source":["## 1. El problema con el texto\n","\n","![](https://drive.google.com/uc?export=view&id=1XkzcdFcmbeIUA9FS2t52daZ6pcOc8Jnz)\n","\n","\n","En esencia, el texto es un tipo de dato **no estructurado** y es necesario darle algún tipo de estructura al momento de llevarlo a un LLM."],"metadata":{"id":"f3leF7-J4GAt"}},{"cell_type":"markdown","source":["## 2. La solución: tokens + embeddings\n","\n","![](https://drive.google.com/uc?export=view&id=1DZxXh612aWcScIJU6TozUzjOjmkfKDMn)\n","\n","En un LLM el texto de entrada debe pasar inicialmente por dos fases de pre-procesamiento:\n","\n","1. **La \"tokenización\"**: donde se subdivide el texto en palabras o sub-palabras y el resultado se representa con un índice numérico\n","2. **La generación de \"embeddings\" de entrada** donde se genera una **representación vectorial inicial** de cada índice numérico\n","\n","Veamos en detalle cada una de estas etapas:"],"metadata":{"id":"tg6wQcFc4ZXz"}},{"cell_type":"markdown","source":["### 2.1. La \"tokenización\"\n","\n","![](https://drive.google.com/uc?export=view&id=1CeGyRyfnazzIU8A5n9o9cwuunrc04qNA)\n","\n","Es la primera fase que permite representar de forma estructurada el texto. Las etapas de esta tokenización son:\n","\n","1. Subdividir el texto en palabras o sub-palabras (esto último es más eficiente que únicamente usar palabras). El resultado son **los tokens**\n","2. Posteriormente, cada token se representa con un identificador numérico único\n","3. Es importante tener en cuenta que cada LLM tiene **tokens especiales** para indicar, por ejemplo, inicio y finalización de frases, padding, palabras desconocidas, etc.\n","\n","Teniendo esto claro, veamos en acción cómo funcionan esta tokenización:"],"metadata":{"id":"OVTYRD7V5hFL"}},{"cell_type":"markdown","source":["Comencemos instalando la librería Transformers de Hugging Face, que nos permitirá acceder a una gran variedad de LLMs:"],"metadata":{"id":"c56XbGm56mKo"}},{"cell_type":"code","source":["!pip install transformers==4.48.3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_7g8rbKQ3AqF","executionInfo":{"status":"ok","timestamp":1741285239626,"user_tz":300,"elapsed":2623,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"c20ec62e-03a3-490d-af98-968b5ca67677"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers==4.48.3 in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.3) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.3) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.3) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.3) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.3) (2025.1.31)\n"]}]},{"cell_type":"markdown","source":["Para usar un LLM generalmente debemos importar dos elementos:\n","\n","- Un \"tokenizer\" para generar los tokens del texto de entrada\n","- Y el modelo\n","\n","> Es importante tener en cuenta que el \"tokenizer\" y el \"modelo\" **están interconectados**, es decir que ambos son creados de manera simultánea al momento de entrenar el modelo"],"metadata":{"id":"_739Js--6uCQ"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MkvhQM_u2N_o","executionInfo":{"status":"ok","timestamp":1741285374610,"user_tz":300,"elapsed":4408,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"4c365868-bc9d-40d2-d483-7b0cc3b96cdf"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]}],"source":["from transformers import AutoModel, AutoTokenizer\n","\n","# Cargar modelo y tokenizador\n","modelo_name = \"microsoft/deberta-v3-xsmall\"\n","tokenizer = AutoTokenizer.from_pretrained(modelo_name)\n","modelo = AutoModel.from_pretrained(modelo_name)"]},{"cell_type":"markdown","source":["Comencemos viendo cómo crear la tokenización:"],"metadata":{"id":"naC5LxfS7K6_"}},{"cell_type":"code","source":["# Texto de entrada al LLM\n","frase = \"banco de madera, banco donde hay dinero\"\n","\n","# Tokenización del texto\n","tokens = tokenizer(frase, return_tensors=\"pt\")\n","\n","# Obtención de los ids correspondientes a cada token\n","token_ids = tokens.input_ids[0]\n","\n","# Comparar número de palabras y de tokens\n","print(\"Número de palabras: \", len(frase.split()))\n","print(\"Número de tokens: \", len(token_ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kljZONo720-A","executionInfo":{"status":"ok","timestamp":1741285543052,"user_tz":300,"elapsed":35,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"e8ac2340-a580-410a-fc53-1ef0b50573cc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Número de palabras:  7\n","Número de tokens:  13\n"]}]},{"cell_type":"markdown","source":["Como se mencionó anteriormente, los tokens pueden ser **palabras o sub-palabras** así que el número de tokens generado siempre será mayor que el número de palabras.\n","\n","> En general esta relación número de tokens / número de palabras puede oscilar entre 1.2 y 2 aproximadamente (todo depende del LLM que estemos usando)\n","\n","Veamos la frase anterior ya tokenizada:"],"metadata":{"id":"HUb4OvWC7beK"}},{"cell_type":"code","source":["token_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EIW7o5yBtkN-","executionInfo":{"status":"ok","timestamp":1741285641010,"user_tz":300,"elapsed":85,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"c8f44544-08b6-4c1f-f4f8-631452375a25"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([     1,   4797,   1902,    718,    412,   3608,    261,   4797,   1902,\n","         66346,  12530, 107725,      2])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Mostrar cada token\n","for id in token_ids:\n","    print(tokenizer.decode(id))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNAGGGVm4scl","executionInfo":{"status":"ok","timestamp":1741285746140,"user_tz":300,"elapsed":45,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"e58ab5aa-56be-4d53-b384-4be68f3e5876"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]\n","ban\n","co\n","de\n","made\n","ra\n",",\n","ban\n","co\n","donde\n","hay\n","dinero\n","[SEP]\n"]}]},{"cell_type":"markdown","source":["¡Y acá vemos precisamente cómo algunos tokens contienen palabras completas y otros sub-palabras!\n","\n","Además podemos observar dos cosas:\n","\n","1. Los signos de puntuación también se tokenizan\n","2. Y el tokenizer incluye automáticamente los tokens especiales cuando sea necesario\n","\n","Veamos la codificación numérica de cada token:"],"metadata":{"id":"zU7Z2QpG8QiA"}},{"cell_type":"code","source":["# Mostrar cada token y su id correspondiente\n","for id in tokens.input_ids[0]:\n","    print(tokenizer.decode(id), \"----->\", id.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b96oONR141iW","executionInfo":{"status":"ok","timestamp":1741285816258,"user_tz":300,"elapsed":46,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"f8c8775c-9387-4751-bf2b-30a1b21ca0ad"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] -----> 1\n","ban -----> 4797\n","co -----> 1902\n","de -----> 718\n","made -----> 412\n","ra -----> 3608\n",", -----> 261\n","ban -----> 4797\n","co -----> 1902\n","donde -----> 66346\n","hay -----> 12530\n","dinero -----> 107725\n","[SEP] -----> 2\n"]}]},{"cell_type":"markdown","source":["En esencia esta codificación se hace a través de un diccionario token -> token_id.\n","\n","El conjunto total de tokens en este diccionario es lo que llamamos **el vocabulario** y el tamaño de este vocabulario será simplemente el número total de tokens diferentes que puede llegar a analizar el modelo:"],"metadata":{"id":"cW5D9lrk8o4Z"}},{"cell_type":"code","source":["# Mostrar el tamaño del vocabulario\n","print(len(tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wb7-ZQvr52dM","executionInfo":{"status":"ok","timestamp":1741285917587,"user_tz":300,"elapsed":8,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"2d1b786e-18d4-438d-8dbb-feeec9e6a829"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["128001\n"]}]},{"cell_type":"markdown","source":["### 2.2. Los \"embeddings\"\n","\n","La representación de cada token a través de \"token_ids\" no es aún adecuada para que el modelo la pueda interpretar.\n","\n","Así que todo LLM tiene un primer bloque de entrada, conocido como **input embedding** (embedding de entrada), que convierte cada \"token_id\" en un vector:\n","\n","![](https://drive.google.com/uc?export=view&id=1GlZTQgZ92AG59xoPhWcI8CJmMs_qfU1x)\n","\n","\n","Los \"embeddings\" cumplen con estos propósitos:\n","\n","1. Estandarizan cada token: sin importar el número de caracteres del token, todos son representados con un vector **de tamaño fijo**\n","2. Los \"embeddings\" generados tienen valores entre -1 y 1 generalmente, lo que los hace adecuados para entrenar y luego generar predicciones con el LLM\n","3. Este primer bloque de generación de \"embeddings\" realiza la codificación de cada token e **intenta preservar su significado o contenido semántico**: palabras **aisladas** con significados similares tendrán \"embeddings\" similares (más adelante estos \"embeddings\" serán refinados usando el mecanismo atencional de los LLMs)\n","\n","Veamos cómo el modelo genera estos \"embeddings\" a la entrada. Comencemos viendo en detalle el modelo que cargamos hace unos momentos:"],"metadata":{"id":"vwv39v4587dc"}},{"cell_type":"code","source":["modelo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mXfanxhE58ms","executionInfo":{"status":"ok","timestamp":1741286458403,"user_tz":300,"elapsed":10,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"4d340d67-26c5-4b64-ecf6-f2b9b0dc6b5f"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DebertaV2Model(\n","  (embeddings): DebertaV2Embeddings(\n","    (word_embeddings): Embedding(128100, 384, padding_idx=0)\n","    (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): DebertaV2Encoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x DebertaV2Layer(\n","        (attention): DebertaV2Attention(\n","          (self): DisentangledSelfAttention(\n","            (query_proj): Linear(in_features=384, out_features=384, bias=True)\n","            (key_proj): Linear(in_features=384, out_features=384, bias=True)\n","            (value_proj): Linear(in_features=384, out_features=384, bias=True)\n","            (pos_dropout): Dropout(p=0.1, inplace=False)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): DebertaV2SelfOutput(\n","            (dense): Linear(in_features=384, out_features=384, bias=True)\n","            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): DebertaV2Intermediate(\n","          (dense): Linear(in_features=384, out_features=1536, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): DebertaV2Output(\n","          (dense): Linear(in_features=1536, out_features=384, bias=True)\n","          (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (rel_embeddings): Embedding(512, 384)\n","    (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n","  )\n",")"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["En el caso anterior podemos ver absolutamente todas las capas que conforman este LLM. Sin embargo, en este caso únicamente nos interesa la capa de entrada:\n","\n","\n","`(word_embeddings): Embedding(128100, 384, padding_idx=0)`\n","\n","Comprendamos el significado de esta porción:\n","\n","- `(word_embeddings)`: el nombre dado a esta capa por los creadores del modelo\n","- `Embedding`: se trata precisamente de una capa que genera embeddings.\n","- `128100`: el tamaño del vocabulario\n","- `384`: el tamaño de cada \"embedding\" generado por este bloque\n","- `padding_idx=0`: indica que los token_ids que tengan valores de cero serán tratados como el token especial de padding (relleno)\n","\n","En esencia una capa de embedding es una simple matriz donde cada fila es un \"embedding\" y cada posición de fila corresponde a un token en particular.\n","\n","Esta matriz se entrena junto con el modelo y por eso es esencial que el tokenizer y el modelo estén sincronizados durante la carga.\n","\n","Así que ahora tomaremos los \"token_ids\" y se los presentaremos únicamente a esta capa de \"embedding\":\n","\n","\n"],"metadata":{"id":"DtJv0NWx-bJx"}},{"cell_type":"code","source":["token_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FfGhZBVGxNJ1","executionInfo":{"status":"ok","timestamp":1741286603382,"user_tz":300,"elapsed":20,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"ed8f4ab7-7783-4b03-bf1f-258e71762b3f"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([     1,   4797,   1902,    718,    412,   3608,    261,   4797,   1902,\n","         66346,  12530, 107725,      2])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# Procesar la secuencia con el modelo y extraer únicamente los word_embeddings\n","import torch\n","\n","# Extract word embeddings\n","with torch.no_grad():\n","    embeddings = modelo.embeddings.word_embeddings(token_ids)\n","\n"],"metadata":{"id":"cTfqFm0u6Ffw","executionInfo":{"status":"ok","timestamp":1741286667324,"user_tz":300,"elapsed":39,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Veamos el tamaño de estos \"embeddings\":"],"metadata":{"id":"VKe5XhlJ_7-w"}},{"cell_type":"code","source":["embeddings.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mee6wOkk6OTn","executionInfo":{"status":"ok","timestamp":1741286681313,"user_tz":300,"elapsed":15,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"2c56d0e5-6178-4063-e302-8329f4a663bb"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([13, 384])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["El tamaño es:\n","\n","- 1: pues hemos presentado sólo una frase al modelo\n","- 15: pues la frase contiene 15 tokens\n","- 384: pues cada token es representado con un \"embedding\" (vector) de 384 elementos"],"metadata":{"id":"NvlltsKN_-Wp"}},{"cell_type":"markdown","source":["Veamos uno de estos embeddings:"],"metadata":{"id":"2EmnbA_gALzh"}},{"cell_type":"code","source":["print(f\"Token: {tokenizer.decode(token_ids[6])} --> {token_ids[6]}\")\n","print(f\"Embedding correspondiente:\")\n","embeddings[6]"],"metadata":{"id":"xAsmRsOKAQjG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741286817115,"user_tz":300,"elapsed":26,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"a9f83da8-4585-4f00-fa3d-2aeeca54bdc4"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Token: , --> 261\n","Embedding correspondiente:\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([ 4.3213e-02, -5.9814e-03,  7.9102e-02,  1.0431e-01,  8.7891e-02,\n","         6.3843e-02,  7.6111e-02,  1.0071e-01,  1.0614e-01,  5.4626e-03,\n","        -1.0437e-02,  4.8065e-02,  9.0027e-04,  4.6814e-02, -2.6855e-03,\n","         2.0630e-02,  1.1719e-02,  9.1309e-02,  9.3689e-03, -6.2988e-02,\n","         1.2299e-01,  9.4177e-02,  1.0095e-01, -5.4382e-02,  2.0752e-03,\n","         5.6030e-02,  3.4790e-02,  6.7627e-02,  1.1749e-02,  5.4688e-02,\n","        -1.4404e-02,  2.9846e-02, -9.0637e-03,  2.6291e-02,  4.0405e-02,\n","        -1.6602e-02, -9.2773e-03,  7.9346e-02,  6.1340e-02,  4.4525e-02,\n","         3.4790e-02,  6.7505e-02,  1.0913e-01,  3.2135e-02,  2.8839e-02,\n","         2.7740e-02,  6.8115e-02,  8.0078e-02,  2.2430e-02, -3.9612e-02,\n","        -1.2781e-01,  6.5613e-02, -8.8257e-02,  2.6520e-02, -7.8467e-01,\n","         2.8259e-02,  1.2451e-01,  4.7150e-02,  2.3438e-02,  1.2207e-02,\n","         8.0444e-02,  3.7781e-02,  4.8950e-02,  4.6875e-02,  6.8909e-02,\n","         4.5715e-02,  9.3018e-02,  9.8877e-03,  4.5967e-03,  1.1243e-01,\n","         4.0009e-02, -3.3569e-02, -6.7383e-02,  4.5410e-02,  3.2654e-02,\n","         4.4800e-02,  1.6785e-02,  1.0788e-02,  1.8784e-02,  8.7402e-02,\n","         8.8745e-02, -3.4027e-02,  8.8257e-02,  1.4496e-02,  9.0393e-02,\n","         7.1838e-02,  1.3306e-01,  9.0088e-02,  3.5706e-02,  1.1719e-01,\n","         1.4984e-02,  9.4666e-02,  1.5271e-01,  2.6001e-02,  6.6956e-02,\n","         1.9714e-02,  8.6792e-02,  1.4709e-02,  6.9763e-02, -5.4321e-03,\n","         1.2109e-01,  4.8767e-02,  8.3679e-02, -8.6975e-03,  9.2285e-02,\n","         6.3965e-02,  3.7292e-02,  8.0322e-02,  2.9663e-02,  1.0889e-01,\n","         5.8472e-02,  7.2815e-02, -3.6316e-02,  7.2693e-02,  1.9653e-02,\n","        -2.3529e-02, -6.8665e-03,  3.8208e-02,  4.2786e-02,  8.1604e-02,\n","         4.6387e-02,  9.2957e-02,  4.6387e-03,  9.5398e-02,  1.1398e-02,\n","         6.5491e-02, -2.9541e-02,  1.8280e-02,  3.4271e-02,  1.0229e-01,\n","         9.4543e-02,  5.5786e-02, -1.0400e-01,  4.7485e-02,  5.6519e-02,\n","         4.0710e-02,  1.6006e-02,  3.7262e-02,  4.8676e-02,  5.8716e-02,\n","         4.1229e-02,  4.4983e-02,  9.2773e-02,  1.3086e-01,  1.2744e+00,\n","         7.7942e-02, -7.6599e-03,  7.8674e-02,  7.8613e-02,  5.6152e-03,\n","         5.4382e-02,  1.1169e-01,  6.2378e-02,  7.2266e-02,  9.6985e-02,\n","         7.2510e-02,  4.7150e-02,  6.5796e-02,  5.1453e-02,  7.6172e-02,\n","         2.2980e-02,  9.3384e-02,  8.3313e-03,  1.0999e-01,  5.5542e-03,\n","         5.4901e-02,  3.0991e-02,  1.1414e-02,  8.7830e-02,  4.3732e-02,\n","         4.5654e-02, -1.9196e-02,  4.4434e-02,  5.4047e-02,  4.9622e-02,\n","         2.3254e-02,  6.0791e-02,  9.0576e-02,  9.0576e-02,  7.5378e-02,\n","         1.3855e-02,  1.1279e-01,  4.8828e-02, -4.3945e-03,  6.4087e-02,\n","         3.7872e-02,  9.5337e-02, -3.1128e-02,  6.9275e-02,  3.7811e-02,\n","         1.8762e-01,  7.1228e-02,  5.9082e-02, -1.0925e-02,  1.0083e-01,\n","         8.9600e-02, -2.0752e-03,  8.3374e-02,  2.2278e-03,  3.8910e-02,\n","        -4.2847e-02,  1.0229e-01,  1.9226e-02,  1.7334e-02,  1.0931e-01,\n","         7.8979e-02,  1.8845e-02,  2.6306e-02, -3.9368e-03,  1.0590e-01,\n","         2.5330e-03,  1.0437e-01,  8.4961e-02, -6.3782e-03,  3.3325e-02,\n","        -1.5137e-01,  6.9702e-02,  8.1421e-02,  2.3102e-02,  5.0812e-02,\n","         3.4851e-02,  4.8981e-02,  3.2471e-02,  6.3843e-02,  7.1411e-02,\n","         7.2144e-02,  4.2389e-02,  4.3213e-02, -1.9638e-02,  8.3435e-02,\n","         1.0889e-01,  7.8857e-02,  8.7646e-02,  4.1718e-02,  1.3672e-02,\n","         8.3252e-02, -3.6926e-03,  6.6833e-02, -6.1707e-02,  8.1421e-02,\n","        -3.6743e-02,  8.5205e-02,  2.2522e-02,  4.1779e-02,  5.2612e-02,\n","        -2.0898e-01,  9.7900e-02,  1.1047e-01,  1.1340e-01,  4.7974e-02,\n","         1.0535e-01,  6.2805e-02,  9.1797e-02, -3.1982e-02, -1.2268e-02,\n","         5.4993e-02,  7.6904e-02,  7.1350e-02,  2.4384e-02,  2.9572e-02,\n","         9.4421e-02,  9.1309e-02,  1.2573e-01,  2.7161e-02,  4.3884e-02,\n","        -8.3160e-04,  9.5291e-03,  7.4463e-02,  7.9285e-02,  7.7026e-02,\n","         6.9824e-02,  6.4880e-02,  8.5449e-02,  5.1056e-02,  1.2122e-01,\n","         4.9866e-02, -6.6566e-03,  2.4170e-02,  1.7380e-02,  1.4771e-01,\n","         4.6448e-02,  4.8309e-02,  1.4236e-02,  6.9153e-02,  9.7168e-02,\n","         1.7749e-01, -6.4453e-02,  1.3594e+00, -1.7059e-02,  6.6956e-02,\n","         5.6549e-02,  7.0862e-02,  9.5215e-02,  7.5073e-02,  1.2134e-01,\n","         7.2708e-03, -6.0394e-02,  2.9907e-02,  7.9468e-02,  5.3040e-02,\n","         1.8616e-02,  8.1482e-03,  3.4363e-02, -1.5030e-02,  1.0144e-01,\n","         8.3862e-02, -4.6143e-02,  3.2715e-02,  4.2480e-02,  4.7455e-03,\n","         4.1687e-02,  1.8951e-02,  2.3865e-02,  1.6357e-02, -1.6815e-02,\n","         8.7097e-02,  5.7800e-02,  8.2642e-02,  2.0355e-02,  5.0659e-02,\n","         5.9845e-02,  4.4800e-02,  8.8928e-02,  5.6396e-02,  7.9407e-02,\n","         7.0190e-02,  8.0872e-02,  6.5979e-02,  7.5195e-02,  3.9032e-02,\n","         5.2521e-02, -6.5735e-02,  5.9547e-03,  2.0050e-02,  8.9661e-02,\n","         5.0354e-02,  6.8726e-02,  8.0566e-02,  6.7627e-02, -1.0620e-02,\n","         5.4596e-02,  4.4708e-02, -4.1962e-03,  6.9641e-02,  5.9082e-02,\n","         8.1482e-02,  9.8999e-02,  3.7231e-03,  7.5134e-02,  7.9773e-02,\n","         6.9458e-02,  9.9609e-02,  1.1115e-01,  2.7588e-02,  4.5868e-02,\n","         2.5330e-02,  8.0750e-02,  1.4648e-02,  4.6875e-02,  5.5145e-02,\n","        -3.1494e-02,  7.1777e-02,  1.2866e-01,  5.2795e-02,  2.9968e-02,\n","         1.7334e-02,  9.3140e-02, -6.1035e-03,  4.4800e-02,  7.6538e-02,\n","         6.5430e-02,  3.1677e-02,  1.1511e-01, -2.0508e-02,  7.4646e-02,\n","         6.6223e-02,  2.8870e-02, -3.0182e-02,  3.6163e-02,  1.0590e-01,\n","         6.3477e-03, -1.0132e-02,  4.7455e-03,  5.5634e-02])"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["Además, si tenemos tokens idénticos (por ejemplo los tokens 2 y 8 son \"ban\") sus \"embeddings\" serán exactamente los mismos:"],"metadata":{"id":"FqkguDTuAoq-"}},{"cell_type":"code","source":["# Tokens 2 y 8\n","print(tokenizer.decode(token_ids[1]))\n","print(tokenizer.decode(token_ids[7]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5UWnU5746V2f","executionInfo":{"status":"ok","timestamp":1741286888191,"user_tz":300,"elapsed":8,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"2f56026c-11b3-4003-9bf9-701545dc8307"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["ban\n","ban\n"]}]},{"cell_type":"code","source":["# Mostrar embeddings correspondientes\n","print(embeddings[1])\n","print(embeddings[7])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qH1PQT4D680X","executionInfo":{"status":"ok","timestamp":1741286907890,"user_tz":300,"elapsed":41,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"dbf17c71-c883-41bd-e937-523df0e64a5a"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-8.8501e-04, -4.3555e-01,  1.2079e-01, -1.9043e-01,  3.7415e-02,\n","        -7.7332e-02, -2.0947e-01,  2.5659e-01, -3.6963e-01, -3.3936e-01,\n","         3.6548e-01,  1.5125e-01,  9.5703e-02,  6.8848e-02, -5.3320e-01,\n","        -2.3438e-01,  1.6382e-01, -2.8369e-01, -3.5620e-01, -4.9194e-02,\n","        -7.3120e-02,  7.1716e-02, -2.2852e-01, -3.4424e-01, -3.1982e-01,\n","        -2.4048e-02, -9.9182e-02,  1.4185e-01, -6.0486e-02, -3.3105e-01,\n","        -3.0304e-02, -5.8624e-02,  6.1584e-02, -3.9600e-01,  1.1877e-01,\n","         1.3123e-02, -1.7676e-01, -9.0881e-02,  3.0005e-01,  3.3783e-02,\n","         7.7515e-02,  1.7102e-01,  8.9539e-02, -1.8884e-01,  3.1445e-01,\n","         9.9243e-02,  1.1719e-01,  4.5557e-01, -1.4893e-02, -1.7078e-01,\n","         6.7993e-02, -2.6001e-01,  1.2732e-01, -1.6205e-02, -1.4160e-02,\n","        -1.5503e-01, -1.6724e-02, -2.1008e-01, -3.9734e-02, -2.9443e-01,\n","         1.8799e-01,  1.2323e-01, -1.1145e-01,  3.5474e-01, -1.4087e-01,\n","        -2.8833e-01, -7.8125e-02,  1.0730e-01, -1.6309e-01, -4.9072e-02,\n","        -1.9165e-01, -5.2307e-02, -1.1523e-01, -1.5002e-01,  4.0454e-01,\n","        -6.1157e-02,  2.8015e-02,  1.0992e-01,  7.1655e-02, -2.7603e-02,\n","        -1.4624e-01,  1.8811e-01, -5.7617e-02, -1.2073e-01,  3.0029e-02,\n","        -1.8970e-01,  2.7985e-02,  1.6296e-01, -6.8237e-02, -2.2766e-01,\n","        -1.3257e-01,  1.5332e-01, -8.3740e-02, -6.7871e-01, -2.3621e-02,\n","        -1.1047e-01, -2.3999e-01,  1.7224e-01,  4.6967e-02,  9.8389e-02,\n","        -4.1162e-01, -1.3574e-01, -7.1594e-02, -3.1006e-02, -1.2384e-01,\n","        -1.7798e-01,  4.7729e-02,  1.9482e-01, -1.9592e-02, -1.4258e-01,\n","         3.1860e-01, -9.7900e-02,  7.1655e-02, -2.3022e-01, -2.4695e-01,\n","         1.3110e-01,  5.7129e-02, -3.3594e-01, -2.4463e-01, -3.7646e-01,\n","        -2.0630e-01,  2.0508e-01, -1.1206e-01,  2.0142e-01,  2.8101e-01,\n","        -2.8174e-01, -4.2786e-02, -8.4961e-02,  1.3794e-01,  4.4128e-02,\n","         4.8157e-02, -1.5967e-01, -1.3428e-03,  4.5801e-01, -2.6465e-01,\n","        -2.2534e-01, -4.0967e-01,  8.4045e-02,  1.2103e-01, -6.4087e-03,\n","        -2.7002e-01, -1.2207e-03,  2.5024e-02,  1.5930e-02, -1.2048e-01,\n","         1.2793e-01, -2.0764e-01, -1.6699e-01,  3.4644e-01, -3.5797e-02,\n","         1.0071e-01, -8.6365e-02, -3.0444e-01, -2.0605e-01,  1.2988e-01,\n","         8.9722e-03,  1.7554e-01,  4.8889e-02, -2.0691e-01,  9.4116e-02,\n","         1.4722e-01,  2.7808e-01, -2.8418e-01, -1.9519e-01, -3.5791e-01,\n","         2.8027e-01, -3.3105e-01, -2.5073e-01, -6.2134e-02, -2.3267e-01,\n","        -9.0942e-02, -5.8105e-01, -5.1758e-02,  4.9023e-01,  3.7354e-02,\n","         1.5820e-01, -9.7778e-02,  2.3413e-01,  2.4109e-01, -4.2725e-04,\n","        -2.8516e-01, -2.0007e-01,  3.6914e-01,  8.7769e-02,  4.6875e-02,\n","         1.2598e-01,  3.3521e-01, -3.0469e-01, -2.3657e-01, -4.2749e-01,\n","        -1.5625e-01,  1.0974e-01, -6.1401e-02,  4.7821e-02, -9.8877e-03,\n","         1.4038e-01,  1.0693e-01,  2.2900e-01, -1.9446e-01, -8.5327e-02,\n","        -1.8311e-02, -2.8296e-01, -2.2241e-01,  7.8979e-02,  4.0283e-02,\n","         3.6719e-01, -2.0422e-01, -2.8638e-01,  1.6553e-01,  2.0264e-01,\n","        -6.6345e-02,  2.5781e-01,  4.4495e-02,  3.2568e-01,  1.5527e-01,\n","         1.5967e-01, -1.7175e-01,  9.8694e-02, -2.8149e-01,  2.4146e-01,\n","         3.6206e-01, -1.4087e-01, -3.2642e-01, -3.4717e-01, -5.5298e-02,\n","         3.1860e-01,  3.0054e-01,  8.7036e-02, -2.4280e-01,  8.8257e-02,\n","         2.6123e-01, -8.5693e-02,  3.0469e-01,  1.7969e-01, -1.4490e-01,\n","        -9.8633e-02,  7.2205e-02, -5.7251e-02, -2.7075e-01, -1.1792e-01,\n","         2.5253e-02, -1.5149e-01,  2.5879e-01,  2.3889e-01,  2.5879e-01,\n","        -7.2327e-03,  1.6858e-01,  5.2643e-02, -3.1006e-02,  2.4670e-01,\n","        -8.3191e-02,  3.2275e-01,  6.6650e-02, -2.8955e-01, -1.6028e-01,\n","         1.6504e-01,  5.1074e-01, -4.7424e-02,  1.3245e-02, -2.2192e-01,\n","         9.3384e-02,  2.6074e-01,  5.7373e-03,  6.0089e-02,  2.1619e-01,\n","         1.9946e-01,  1.1841e-02,  4.7803e-01,  1.5454e-01, -4.1235e-01,\n","         2.8906e-01,  9.9426e-02,  3.0737e-01, -1.6504e-01,  4.6387e-03,\n","         2.6917e-02, -1.1420e-01,  3.7140e-02, -3.0835e-01,  1.7773e-01,\n","         2.5049e-01,  1.2378e-01,  1.9531e-01, -2.0740e-01, -4.4800e-02,\n","        -4.5471e-02,  2.8778e-02,  3.2666e-01,  4.0820e-01,  3.6621e-01,\n","         7.7576e-02,  9.0637e-02, -3.0200e-01, -1.1633e-01,  2.2302e-01,\n","        -6.4636e-02, -2.2864e-01,  1.1536e-02,  3.2617e-01, -1.9153e-01,\n","        -5.3772e-02, -4.0527e-01, -3.7109e-02, -1.4307e-01, -1.5405e-01,\n","         2.3047e-01,  2.1313e-01,  1.4966e-01,  1.6052e-02,  1.0803e-01,\n","         7.4234e-03,  3.1787e-01, -7.3242e-04, -2.1936e-01, -2.5415e-01,\n","        -4.0283e-02, -6.5979e-02, -2.7856e-01,  2.1045e-01,  2.2385e-02,\n","        -3.3228e-01,  2.2754e-01, -6.5186e-02,  7.3181e-02, -1.9592e-01,\n","        -9.4238e-02, -1.3892e-01,  9.9182e-02, -3.3569e-03,  8.4839e-02,\n","        -3.3105e-01, -9.1919e-02,  1.0217e-01, -7.5500e-02,  3.0957e-01,\n","        -7.1716e-02, -4.7211e-02,  5.3467e-02,  3.1006e-02, -3.3765e-01,\n","         1.8091e-01, -5.4626e-02, -1.1792e-01, -5.0598e-02, -2.4918e-02,\n","        -3.1311e-02, -1.4319e-01, -2.7466e-03,  2.7515e-01,  1.6943e-01,\n","        -6.5308e-02,  1.4172e-01, -1.2512e-01,  4.2383e-01,  3.3325e-02,\n","        -1.5869e-02, -7.9346e-04, -1.6992e-01,  4.4580e-01, -2.5513e-02,\n","        -6.1035e-02,  1.6394e-01, -1.0742e-01, -1.0536e-02,  2.0740e-01,\n","        -1.3696e-01, -3.6621e-01, -9.3628e-02,  1.2042e-01, -1.3696e-01,\n","         4.0771e-02,  2.9321e-01,  9.5215e-03, -2.8369e-01,  1.2250e-01,\n","         6.6528e-02, -1.9873e-01, -2.9077e-01, -1.8286e-01,  1.4172e-01,\n","         2.1558e-01, -3.6572e-01, -2.6169e-02, -1.3159e-01])\n","tensor([-8.8501e-04, -4.3555e-01,  1.2079e-01, -1.9043e-01,  3.7415e-02,\n","        -7.7332e-02, -2.0947e-01,  2.5659e-01, -3.6963e-01, -3.3936e-01,\n","         3.6548e-01,  1.5125e-01,  9.5703e-02,  6.8848e-02, -5.3320e-01,\n","        -2.3438e-01,  1.6382e-01, -2.8369e-01, -3.5620e-01, -4.9194e-02,\n","        -7.3120e-02,  7.1716e-02, -2.2852e-01, -3.4424e-01, -3.1982e-01,\n","        -2.4048e-02, -9.9182e-02,  1.4185e-01, -6.0486e-02, -3.3105e-01,\n","        -3.0304e-02, -5.8624e-02,  6.1584e-02, -3.9600e-01,  1.1877e-01,\n","         1.3123e-02, -1.7676e-01, -9.0881e-02,  3.0005e-01,  3.3783e-02,\n","         7.7515e-02,  1.7102e-01,  8.9539e-02, -1.8884e-01,  3.1445e-01,\n","         9.9243e-02,  1.1719e-01,  4.5557e-01, -1.4893e-02, -1.7078e-01,\n","         6.7993e-02, -2.6001e-01,  1.2732e-01, -1.6205e-02, -1.4160e-02,\n","        -1.5503e-01, -1.6724e-02, -2.1008e-01, -3.9734e-02, -2.9443e-01,\n","         1.8799e-01,  1.2323e-01, -1.1145e-01,  3.5474e-01, -1.4087e-01,\n","        -2.8833e-01, -7.8125e-02,  1.0730e-01, -1.6309e-01, -4.9072e-02,\n","        -1.9165e-01, -5.2307e-02, -1.1523e-01, -1.5002e-01,  4.0454e-01,\n","        -6.1157e-02,  2.8015e-02,  1.0992e-01,  7.1655e-02, -2.7603e-02,\n","        -1.4624e-01,  1.8811e-01, -5.7617e-02, -1.2073e-01,  3.0029e-02,\n","        -1.8970e-01,  2.7985e-02,  1.6296e-01, -6.8237e-02, -2.2766e-01,\n","        -1.3257e-01,  1.5332e-01, -8.3740e-02, -6.7871e-01, -2.3621e-02,\n","        -1.1047e-01, -2.3999e-01,  1.7224e-01,  4.6967e-02,  9.8389e-02,\n","        -4.1162e-01, -1.3574e-01, -7.1594e-02, -3.1006e-02, -1.2384e-01,\n","        -1.7798e-01,  4.7729e-02,  1.9482e-01, -1.9592e-02, -1.4258e-01,\n","         3.1860e-01, -9.7900e-02,  7.1655e-02, -2.3022e-01, -2.4695e-01,\n","         1.3110e-01,  5.7129e-02, -3.3594e-01, -2.4463e-01, -3.7646e-01,\n","        -2.0630e-01,  2.0508e-01, -1.1206e-01,  2.0142e-01,  2.8101e-01,\n","        -2.8174e-01, -4.2786e-02, -8.4961e-02,  1.3794e-01,  4.4128e-02,\n","         4.8157e-02, -1.5967e-01, -1.3428e-03,  4.5801e-01, -2.6465e-01,\n","        -2.2534e-01, -4.0967e-01,  8.4045e-02,  1.2103e-01, -6.4087e-03,\n","        -2.7002e-01, -1.2207e-03,  2.5024e-02,  1.5930e-02, -1.2048e-01,\n","         1.2793e-01, -2.0764e-01, -1.6699e-01,  3.4644e-01, -3.5797e-02,\n","         1.0071e-01, -8.6365e-02, -3.0444e-01, -2.0605e-01,  1.2988e-01,\n","         8.9722e-03,  1.7554e-01,  4.8889e-02, -2.0691e-01,  9.4116e-02,\n","         1.4722e-01,  2.7808e-01, -2.8418e-01, -1.9519e-01, -3.5791e-01,\n","         2.8027e-01, -3.3105e-01, -2.5073e-01, -6.2134e-02, -2.3267e-01,\n","        -9.0942e-02, -5.8105e-01, -5.1758e-02,  4.9023e-01,  3.7354e-02,\n","         1.5820e-01, -9.7778e-02,  2.3413e-01,  2.4109e-01, -4.2725e-04,\n","        -2.8516e-01, -2.0007e-01,  3.6914e-01,  8.7769e-02,  4.6875e-02,\n","         1.2598e-01,  3.3521e-01, -3.0469e-01, -2.3657e-01, -4.2749e-01,\n","        -1.5625e-01,  1.0974e-01, -6.1401e-02,  4.7821e-02, -9.8877e-03,\n","         1.4038e-01,  1.0693e-01,  2.2900e-01, -1.9446e-01, -8.5327e-02,\n","        -1.8311e-02, -2.8296e-01, -2.2241e-01,  7.8979e-02,  4.0283e-02,\n","         3.6719e-01, -2.0422e-01, -2.8638e-01,  1.6553e-01,  2.0264e-01,\n","        -6.6345e-02,  2.5781e-01,  4.4495e-02,  3.2568e-01,  1.5527e-01,\n","         1.5967e-01, -1.7175e-01,  9.8694e-02, -2.8149e-01,  2.4146e-01,\n","         3.6206e-01, -1.4087e-01, -3.2642e-01, -3.4717e-01, -5.5298e-02,\n","         3.1860e-01,  3.0054e-01,  8.7036e-02, -2.4280e-01,  8.8257e-02,\n","         2.6123e-01, -8.5693e-02,  3.0469e-01,  1.7969e-01, -1.4490e-01,\n","        -9.8633e-02,  7.2205e-02, -5.7251e-02, -2.7075e-01, -1.1792e-01,\n","         2.5253e-02, -1.5149e-01,  2.5879e-01,  2.3889e-01,  2.5879e-01,\n","        -7.2327e-03,  1.6858e-01,  5.2643e-02, -3.1006e-02,  2.4670e-01,\n","        -8.3191e-02,  3.2275e-01,  6.6650e-02, -2.8955e-01, -1.6028e-01,\n","         1.6504e-01,  5.1074e-01, -4.7424e-02,  1.3245e-02, -2.2192e-01,\n","         9.3384e-02,  2.6074e-01,  5.7373e-03,  6.0089e-02,  2.1619e-01,\n","         1.9946e-01,  1.1841e-02,  4.7803e-01,  1.5454e-01, -4.1235e-01,\n","         2.8906e-01,  9.9426e-02,  3.0737e-01, -1.6504e-01,  4.6387e-03,\n","         2.6917e-02, -1.1420e-01,  3.7140e-02, -3.0835e-01,  1.7773e-01,\n","         2.5049e-01,  1.2378e-01,  1.9531e-01, -2.0740e-01, -4.4800e-02,\n","        -4.5471e-02,  2.8778e-02,  3.2666e-01,  4.0820e-01,  3.6621e-01,\n","         7.7576e-02,  9.0637e-02, -3.0200e-01, -1.1633e-01,  2.2302e-01,\n","        -6.4636e-02, -2.2864e-01,  1.1536e-02,  3.2617e-01, -1.9153e-01,\n","        -5.3772e-02, -4.0527e-01, -3.7109e-02, -1.4307e-01, -1.5405e-01,\n","         2.3047e-01,  2.1313e-01,  1.4966e-01,  1.6052e-02,  1.0803e-01,\n","         7.4234e-03,  3.1787e-01, -7.3242e-04, -2.1936e-01, -2.5415e-01,\n","        -4.0283e-02, -6.5979e-02, -2.7856e-01,  2.1045e-01,  2.2385e-02,\n","        -3.3228e-01,  2.2754e-01, -6.5186e-02,  7.3181e-02, -1.9592e-01,\n","        -9.4238e-02, -1.3892e-01,  9.9182e-02, -3.3569e-03,  8.4839e-02,\n","        -3.3105e-01, -9.1919e-02,  1.0217e-01, -7.5500e-02,  3.0957e-01,\n","        -7.1716e-02, -4.7211e-02,  5.3467e-02,  3.1006e-02, -3.3765e-01,\n","         1.8091e-01, -5.4626e-02, -1.1792e-01, -5.0598e-02, -2.4918e-02,\n","        -3.1311e-02, -1.4319e-01, -2.7466e-03,  2.7515e-01,  1.6943e-01,\n","        -6.5308e-02,  1.4172e-01, -1.2512e-01,  4.2383e-01,  3.3325e-02,\n","        -1.5869e-02, -7.9346e-04, -1.6992e-01,  4.4580e-01, -2.5513e-02,\n","        -6.1035e-02,  1.6394e-01, -1.0742e-01, -1.0536e-02,  2.0740e-01,\n","        -1.3696e-01, -3.6621e-01, -9.3628e-02,  1.2042e-01, -1.3696e-01,\n","         4.0771e-02,  2.9321e-01,  9.5215e-03, -2.8369e-01,  1.2250e-01,\n","         6.6528e-02, -1.9873e-01, -2.9077e-01, -1.8286e-01,  1.4172e-01,\n","         2.1558e-01, -3.6572e-01, -2.6169e-02, -1.3159e-01])\n"]}]},{"cell_type":"markdown","source":["Y verifiquemos que estos \"embeddings\" son idénticos:"],"metadata":{"id":"30J6ILZcBAzh"}},{"cell_type":"code","source":["# Mostrar resta de estos embeddings\n","print(embeddings[1] - embeddings[7])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w8bcrKkO7CK2","executionInfo":{"status":"ok","timestamp":1741286926731,"user_tz":300,"elapsed":12,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"891bb99b-7a93-4b25-a712-52fbf3d8878e"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"]}]},{"cell_type":"markdown","source":["## 3. La ventana de contexto\n","\n","> Es simplemente el número máximo de tokens que podrá procesar el LLM\n","\n","Así que en esencia, la ventana de contexto define el tamaño máximo que podrá tener la secuencia para que el modelo la pueda procesar.\n","\n","Este tamaño lo podemos encontrar accediendo a los detalles de la configuración del modelo:"],"metadata":{"id":"oVHL3jlFBFsz"}},{"cell_type":"code","source":["from transformers import AutoConfig\n","\n","# Configuración del modelo\n","config = AutoConfig.from_pretrained(modelo_name)\n","\n","# Print max sequence length\n","print(f\"Ventana de contexto: {config.max_position_embeddings}\")\n"],"metadata":{"id":"DWnRZgUt7FtN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741287086675,"user_tz":300,"elapsed":80,"user":{"displayName":"Miguel Sotaquirá","userId":"16233284746872182672"}},"outputId":"4e136c6a-645f-4944-d705-1983b66affdd"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Ventana de contexto: 512\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"C959gI60on4P"},"execution_count":null,"outputs":[]}]}